# Credit Risk Model Stability

A machine learning pipeline for predicting credit risk with a focus on **temporal model stability**. Trains CatBoost, LightGBM, and XGBoost base models, tunes them via Optuna with the Kaggle Gini stability metric as the objective, and stacks them through a calibrated Ridge meta-learner. Includes a Streamlit dashboard for drift analysis, stability monitoring, and interactive risk prediction.

## Project Structure

```
.
├── app/
│   └── app.py                 # Streamlit dashboard (3 pages)
├── artifacts/                  # Generated by the training pipeline
│   ├── best_params.json
│   └── tuned/
│       ├── catboost_fold_*.cbm
│       ├── lgb_fold_*.txt
│       ├── xgb_fold_*.json
│       ├── xgb_encoder_fold_*.pkl
│       ├── meta_learner_seed_*.pkl
│       ├── feature_config.json
│       ├── tuned_oof.parquet
│       ├── ensemble_oof.parquet
│       └── *.json              # Score summaries
├── data/
│   ├── parquet_files/
│   │   ├── train/
│   │   └── test/
│   └── processed/              # Generated feature parquets
├── guidelines/
│   └── commits.md
├── notebooks/
│   ├── exploration.ipynb       # Full EDA + training pipeline
│   └── submission.ipynb        # Self-contained Kaggle submission
├── src/
│   ├── data_processing.py      # Parquet loading, downcasting, cleaning
│   ├── features.py             # Date transforms, ratios, aggregations
│   ├── metrics.py              # Kaggle Gini stability metric
│   ├── tuning.py               # Optuna stability-aware HP tuning
│   └── ensemble.py             # Stacking meta-learner
├── requirements.txt
└── README.md
```

## Setup

```bash
# Clone the repository
git clone <repo-url>
cd credit-risk-model-stability

# Create a virtual environment
python -m venv env
source env/bin/activate        # Linux / macOS
# env\Scripts\activate         # Windows

# Install dependencies
pip install -r requirements.txt
```

Place the competition data so that `data/parquet_files/train/` and `data/parquet_files/test/` contain the raw parquet files.

## Training Pipeline

Run the full pipeline in `notebooks/exploration.ipynb` **cell by cell** (or "Run All"). The notebook executes the following stages in order:

1. **Data loading & EDA** — loads base + static tables, profiles missing rates and temporal drift.
2. **Feature engineering** — date transforms, domain ratios, depth-1 and depth-2 aggregations across all table groups.
3. **Post-merge filtering** — drops >95 %-correlated columns, collapses rare categories, removes drift-prone features.
4. **Baseline models** — trains CatBoost, LightGBM, and XGBoost with 5-fold `StratifiedGroupKFold` (grouped by `WEEK_NUM`).
5. **Optuna tuning** — maximises the Gini stability metric (not AUC). Best params saved to `artifacts/best_params.json`.
6. **Retrain with tuned params** — regenerates OOF and test predictions. Saves fold models, feature lists, and encoders to `artifacts/tuned/`.
7. **Stacking ensemble** — `CalibratedClassifierCV(RidgeClassifier)` meta-learner with 3-seed averaging.
8. **Holdout & profiling** — pseudo-future holdout (80/20 week split) and runtime budget analysis.

After running the notebook, `artifacts/tuned/` will contain all model files, OOF predictions, and configuration needed by the Streamlit app and submission notebook.

## Kaggle Submission

`notebooks/submission.ipynb` is a **self-contained** notebook that reproduces the entire pipeline without any `src/` imports or Optuna tuning loops:

```
notebooks/submission.ipynb
  → Loads raw parquet data
  → Builds features (depth-0 / 1 / 2)
  → Trains CatBoost + LightGBM + XGBoost (fixed tuned params)
  → Stacks via Ridge meta-learner
  → Validates and saves submission.csv
```

**To use on Kaggle:**

1. Upload the notebook to a Kaggle competition kernel.
2. Attach the competition dataset.
3. Tuned hyperparameters are embedded as defaults. To use your own Optuna results, either edit the `TUNED_PARAMS` dict at the top of the notebook, or upload `artifacts/best_params.json` as a Kaggle dataset — the notebook auto-loads it when available.
4. Run All. `submission.csv` is written to the working directory.

**Validation checks** run before writing `submission.csv`:
- Columns are exactly `['case_id', 'score']`
- Row count matches the test base table
- No duplicate `case_id` values
- No null scores
- All scores clipped to [0, 1]

## Streamlit Dashboard

```bash
streamlit run app/app.py
```

The app has three pages accessible via the sidebar:

| Page | Description | Data source |
|------|-------------|-------------|
| **Data Drift** | Feature distributions across WEEK_NUM bins, KS-test drift scores, severity table | Raw or processed training data |
| **Model Stability** | Weekly Gini curves, trend regression, falling-rate / residual metrics, model-vs-ensemble comparison | `artifacts/tuned/tuned_oof.parquet` |
| **Risk Predictor** | Enter applicant financials, get predicted default probability and risk tier | `artifacts/tuned/catboost_fold_*.cbm` |

- **Data Drift** works with raw data and requires no artifacts.
- **Model Stability** and **Risk Predictor** require that the training pipeline has been run first so that model artifacts exist under `artifacts/tuned/`.

## Evaluation Metric

The competition uses the **Gini stability metric**:

```
stability = mean(gini) + 88 * min(0, slope) - 0.5 * std(residuals)
```

where `gini = 2 * AUC - 1` is computed per `WEEK_NUM`, and `slope` / `residuals` come from a linear regression fit through the weekly Gini values. The metric penalises models whose performance degrades over time (negative slope) or fluctuates erratically (high residual variance).
