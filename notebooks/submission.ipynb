{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model Stability — Submission Notebook\n",
    "\n",
    "Self-contained Kaggle-submittable notebook. Reproduces the full pipeline:\n",
    "\n",
    "1. Load raw parquet data\n",
    "2. Build features (depth-0, depth-1, depth-2 aggregations)\n",
    "3. Train tuned CatBoost / LightGBM / XGBoost with 5-fold StratifiedGroupKFold\n",
    "4. Stack via CalibratedClassifierCV(RidgeClassifier) meta-learner\n",
    "5. Save `submission.csv`\n",
    "\n",
    "**No Optuna tuning loops** — uses fixed, pre-tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────\n",
    "KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "DATA_PATH = Path(\n",
    "    \"/kaggle/input/home-credit-credit-risk-model-stability\"\n",
    "    if KAGGLE else \"../data\"\n",
    ")\n",
    "\n",
    "# ── Tuned hyperparameters ────────────────────────────────────────\n",
    "# Defaults below; overwritten by artifacts/best_params.json when available.\n",
    "TUNED_PARAMS = {\n",
    "    \"catboost\": {\n",
    "        \"depth\": 6, \"learning_rate\": 0.05, \"l2_leaf_reg\": 3.0,\n",
    "        \"subsample\": 0.8, \"colsample_bylevel\": 0.8,\n",
    "        \"bootstrap_type\": \"Bernoulli\",\n",
    "    },\n",
    "    \"lightgbm\": {\n",
    "        \"max_depth\": 7, \"num_leaves\": 64, \"learning_rate\": 0.05,\n",
    "        \"reg_lambda\": 3.0, \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        \"max_depth\": 6, \"learning_rate\": 0.05, \"reg_lambda\": 3.0,\n",
    "        \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n",
    "    },\n",
    "}\n",
    "\n",
    "params_path = Path(\"../artifacts/best_params.json\") if not KAGGLE else None\n",
    "if params_path and params_path.exists():\n",
    "    with open(params_path) as f:\n",
    "        saved = json.load(f)\n",
    "    for model_name in TUNED_PARAMS:\n",
    "        if model_name in saved and \"best_params\" in saved[model_name]:\n",
    "            TUNED_PARAMS[model_name] = saved[model_name][\"best_params\"]\n",
    "    print(\"Loaded tuned params from\", params_path)\n",
    "else:\n",
    "    print(\"Using default tuned params (update from Optuna results)\")\n",
    "\n",
    "for name, params in TUNED_PARAMS.items():\n",
    "    print(f\"  {name}: {params}\")\n",
    "\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "ENSEMBLE_SEEDS = [42, 123, 456]\n",
    "ENSEMBLE_ALPHA = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table_group(data_path, table_name, split=\"train\"):\n",
    "    \"\"\"Load and concatenate all parquet files for a table group.\"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    full_dir = data_path / \"parquet_files\" / split\n",
    "    pattern = f\"{split}_{table_name}\"\n",
    "    matching_files = sorted(full_dir.glob(f\"{pattern}*.parquet\"))\n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No files: '{pattern}*.parquet' in {full_dir}\")\n",
    "    dfs = [pl.read_parquet(f) for f in matching_files]\n",
    "    return pl.concat(dfs, how=\"vertical_relaxed\") if len(dfs) > 1 else dfs[0]\n",
    "\n",
    "\n",
    "def preprocess_table(df, missing_threshold=0.98, max_string_cardinality=10_000):\n",
    "    \"\"\"Downcast numerics, drop high-missing and high-cardinality string columns.\"\"\"\n",
    "    casts = []\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        if dtype == pl.Float64:\n",
    "            casts.append(pl.col(col).cast(pl.Float32))\n",
    "        elif dtype == pl.Int64:\n",
    "            casts.append(pl.col(col).cast(pl.Int32))\n",
    "        else:\n",
    "            casts.append(pl.col(col))\n",
    "    df = df.select(casts)\n",
    "\n",
    "    n = df.height\n",
    "    if n > 0:\n",
    "        null_rates = df.null_count() / n\n",
    "        df = df.select([c for c in df.columns if null_rates[c][0] <= missing_threshold])\n",
    "\n",
    "    to_drop = [\n",
    "        c for c in df.columns\n",
    "        if df[c].dtype in (pl.String, pl.Utf8) and df[c].n_unique() > max_string_cardinality\n",
    "    ]\n",
    "    return df.drop(to_drop) if to_drop else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_DTYPES = frozenset({\n",
    "    pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "    pl.Float32, pl.Float64,\n",
    "})\n",
    "STRING_DTYPES = frozenset({pl.String, pl.Utf8, pl.Categorical})\n",
    "\n",
    "\n",
    "def handle_dates(df):\n",
    "    \"\"\"Transform date/year columns to numeric features relative to date_decision.\"\"\"\n",
    "    if \"date_decision\" not in df.columns:\n",
    "        return df\n",
    "    date_d_cols = [c for c in df.columns if c.endswith(\"D\") and c != \"date_decision\"]\n",
    "    year_cols = [\n",
    "        c for c in df.columns\n",
    "        if \"year\" in c.lower() and c not in date_d_cols and c != \"date_decision\"\n",
    "    ]\n",
    "    exprs = []\n",
    "    for col in date_d_cols:\n",
    "        exprs.append(\n",
    "            ((pl.col(col) - pl.col(\"date_decision\")).dt.total_days() / -365)\n",
    "            .cast(pl.Float32).alias(col)\n",
    "        )\n",
    "    for col in year_cols:\n",
    "        exprs.append(\n",
    "            (pl.col(col) - pl.col(\"date_decision\").dt.year())\n",
    "            .cast(pl.Float32).alias(col)\n",
    "        )\n",
    "    if exprs:\n",
    "        df = df.with_columns(exprs)\n",
    "    return df.drop([c for c in (\"date_decision\", \"MONTH\") if c in df.columns])\n",
    "\n",
    "\n",
    "def create_domain_ratios(df):\n",
    "    \"\"\"Create loan burden, disbursement, debt, and EIR ratio features.\"\"\"\n",
    "    COL = {\n",
    "        \"price\": \"price_1097A\", \"annuity\": \"annuity_780A\",\n",
    "        \"disbursed\": \"disbursedcredamount_1113A\", \"credit_amount\": \"credamount_770A\",\n",
    "        \"total_debt\": \"totaldebt_9A\", \"eir\": \"eir_270L\",\n",
    "    }\n",
    "    avail = {k: v for k, v in COL.items() if v in df.columns}\n",
    "    ratios = []\n",
    "    if {\"price\", \"annuity\"} <= avail.keys():\n",
    "        ratios.append(\n",
    "            (pl.col(avail[\"price\"]) / pl.col(avail[\"annuity\"]))\n",
    "            .cast(pl.Float32).alias(\"loan_burden_ratio\"))\n",
    "    if {\"disbursed\", \"credit_amount\"} <= avail.keys():\n",
    "        ratios.append(\n",
    "            (pl.col(avail[\"disbursed\"]) / pl.col(avail[\"credit_amount\"]))\n",
    "            .cast(pl.Float32).alias(\"disbursed_credit_ratio\"))\n",
    "    if {\"total_debt\", \"credit_amount\"} <= avail.keys():\n",
    "        ratios.append(\n",
    "            (pl.col(avail[\"total_debt\"]) / (1 + pl.col(avail[\"credit_amount\"])))\n",
    "            .cast(pl.Float32).alias(\"debt_credit_ratio\"))\n",
    "    if {\"eir\", \"credit_amount\"} <= avail.keys():\n",
    "        ratios.append(\n",
    "            (pl.col(avail[\"eir\"]) / pl.col(avail[\"credit_amount\"]))\n",
    "            .cast(pl.Float32).alias(\"eir_credit_ratio\"))\n",
    "    return df.with_columns(ratios) if ratios else df\n",
    "\n",
    "\n",
    "def _build_agg_exprs(df, skip):\n",
    "    \"\"\"Classify columns and build Polars aggregation expressions.\"\"\"\n",
    "    n_rows = df.height\n",
    "    numeric_cols, amount_cols, string_mode_cols = [], [], []\n",
    "    cat_count_map = {}\n",
    "    for col in df.columns:\n",
    "        if col in skip:\n",
    "            continue\n",
    "        dtype = df[col].dtype\n",
    "        if dtype in NUMERIC_DTYPES:\n",
    "            numeric_cols.append(col)\n",
    "            if col.endswith(\"A\"):\n",
    "                amount_cols.append(col)\n",
    "        elif dtype in STRING_DTYPES:\n",
    "            n_uniq = df[col].n_unique()\n",
    "            null_rate = df[col].null_count() / n_rows if n_rows > 0 else 1.0\n",
    "            if n_uniq <= 200:\n",
    "                string_mode_cols.append(col)\n",
    "            if n_uniq <= 10 and null_rate < 0.9:\n",
    "                cat_count_map[col] = df[col].drop_nulls().unique().to_list()\n",
    "    agg = []\n",
    "    for col in numeric_cols:\n",
    "        agg.extend([\n",
    "            pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "            pl.col(col).max().alias(f\"{col}_max\"),\n",
    "            pl.col(col).min().alias(f\"{col}_min\"),\n",
    "            pl.col(col).first().alias(f\"{col}_first\"),\n",
    "            pl.col(col).last().alias(f\"{col}_last\"),\n",
    "            pl.col(col).std().alias(f\"{col}_std\"),\n",
    "        ])\n",
    "    for col in amount_cols:\n",
    "        agg.append(\n",
    "            (pl.col(col).std() / (pl.col(col).mean().abs() + 1e-9))\n",
    "            .alias(f\"{col}_cv\"))\n",
    "    for col in string_mode_cols:\n",
    "        agg.extend([\n",
    "            pl.col(col).drop_nulls().mode().first().alias(f\"{col}_mode\"),\n",
    "            pl.col(col).n_unique().alias(f\"{col}_nunique\"),\n",
    "        ])\n",
    "    for col, vals in cat_count_map.items():\n",
    "        for val in vals:\n",
    "            safe = str(val).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            agg.append((pl.col(col) == val).sum().alias(f\"{col}_{safe}_count\"))\n",
    "    return agg\n",
    "\n",
    "\n",
    "def aggregate_depth1(df, group_col=\"case_id\"):\n",
    "    \"\"\"Aggregate a depth-1 table by case_id after sorting by num_group1.\"\"\"\n",
    "    skip = {group_col, \"num_group1\", \"num_group2\"}\n",
    "    if \"num_group1\" in df.columns:\n",
    "        df = df.sort(group_col, \"num_group1\")\n",
    "    agg = _build_agg_exprs(df, skip)\n",
    "    return df.group_by(group_col).agg(agg) if agg else df.select(group_col).unique()\n",
    "\n",
    "\n",
    "def aggregate_depth2(df):\n",
    "    \"\"\"Two-pass aggregation for depth-2 tables (num_group2 then num_group1).\"\"\"\n",
    "    skip1 = {\"case_id\", \"num_group1\", \"num_group2\"}\n",
    "    if \"num_group2\" in df.columns:\n",
    "        df = df.sort(\"case_id\", \"num_group1\", \"num_group2\")\n",
    "    agg1 = _build_agg_exprs(df, skip1)\n",
    "    if not agg1:\n",
    "        return df.select(\"case_id\").unique()\n",
    "    pass1 = df.group_by([\"case_id\", \"num_group1\"]).agg(agg1)\n",
    "    pass1 = pass1.sort(\"case_id\", \"num_group1\")\n",
    "    agg2 = _build_agg_exprs(pass1, {\"case_id\", \"num_group1\"})\n",
    "    return pass1.group_by(\"case_id\").agg(agg2) if agg2 else pass1.select(\"case_id\").unique()\n",
    "\n",
    "\n",
    "def drop_correlated_columns(df, threshold=0.95, sample_n=50_000):\n",
    "    \"\"\"Drop one column from each pair whose |Pearson r| > threshold.\"\"\"\n",
    "    protect = {\"case_id\", \"target\", \"WEEK_NUM\"}\n",
    "    num_cols = [c for c in df.columns if df[c].dtype in NUMERIC_DTYPES and c not in protect]\n",
    "    if len(num_cols) < 2:\n",
    "        return df\n",
    "    sub = df.select(num_cols)\n",
    "    if sub.height > sample_n:\n",
    "        sub = sub.sample(n=sample_n, seed=42)\n",
    "    mat = sub.fill_null(0).to_numpy().astype(np.float32)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.abs(np.corrcoef(mat, rowvar=False))\n",
    "    np.nan_to_num(corr, copy=False, nan=0.0)\n",
    "    null_rates = np.array([df[c].null_count() / df.height for c in num_cols])\n",
    "    to_drop = set()\n",
    "    for i in range(len(num_cols)):\n",
    "        if i in to_drop:\n",
    "            continue\n",
    "        for j in range(i + 1, len(num_cols)):\n",
    "            if j in to_drop:\n",
    "                continue\n",
    "            if corr[i, j] > threshold:\n",
    "                if null_rates[i] >= null_rates[j]:\n",
    "                    to_drop.add(i)\n",
    "                    break\n",
    "                else:\n",
    "                    to_drop.add(j)\n",
    "    drop_names = sorted(num_cols[i] for i in to_drop)\n",
    "    if drop_names:\n",
    "        print(f\"  Dropped {len(drop_names)} correlated columns (|r|>{threshold})\")\n",
    "    return df.drop(drop_names) if drop_names else df\n",
    "\n",
    "\n",
    "def collapse_rare_categories(df, max_unique=200, keep_top=20):\n",
    "    \"\"\"For high-cardinality string columns, keep top values, null the rest.\"\"\"\n",
    "    exprs = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype not in STRING_DTYPES:\n",
    "            continue\n",
    "        if df[col].n_unique() <= max_unique:\n",
    "            continue\n",
    "        top = (\n",
    "            df[col].value_counts().sort(\"count\", descending=True)\n",
    "            .head(keep_top)[col].to_list()\n",
    "        )\n",
    "        exprs.append(\n",
    "            pl.when(pl.col(col).is_in(top)).then(pl.col(col))\n",
    "            .otherwise(None).alias(col)\n",
    "        )\n",
    "    return df.with_columns(exprs) if exprs else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(week_num, target, score, w_falling=88.0, w_std=0.5):\n",
    "    \"\"\"Kaggle Gini stability metric: mean(gini) + 88*min(0,slope) - 0.5*std(resid).\"\"\"\n",
    "    weeks, y_true, y_score = np.asarray(week_num), np.asarray(target), np.asarray(score)\n",
    "    ginis = []\n",
    "    for w in np.sort(np.unique(weeks)):\n",
    "        mask = weeks == w\n",
    "        if len(np.unique(y_true[mask])) < 2:\n",
    "            continue\n",
    "        ginis.append(2.0 * roc_auc_score(y_true[mask], y_score[mask]) - 1.0)\n",
    "    ginis = np.array(ginis)\n",
    "    mean_gini = float(np.mean(ginis)) if len(ginis) else 0.0\n",
    "    if len(ginis) < 2:\n",
    "        return {\"stability_score\": mean_gini, \"mean_gini\": mean_gini}\n",
    "    x = np.arange(len(ginis))\n",
    "    slope, intercept = np.polyfit(x, ginis, 1)\n",
    "    residuals = ginis - (slope * x + intercept)\n",
    "    return {\n",
    "        \"stability_score\": float(mean_gini + w_falling * min(0.0, slope) - w_std * np.std(residuals)),\n",
    "        \"mean_gini\": mean_gini,\n",
    "        \"falling_rate\": float(min(0.0, slope)),\n",
    "        \"std_residuals\": float(np.std(residuals)),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_stacking_ensemble(oof_scores, y, week_num, test_scores,\n",
    "                            seeds=None, alpha=1.0, n_splits=5, cv_seed=42):\n",
    "    \"\"\"Stack base-model OOF predictions via CalibratedClassifierCV(RidgeClassifier).\"\"\"\n",
    "    seeds = seeds or [42, 123, 456]\n",
    "    outer_cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=cv_seed)\n",
    "    oof_ens = np.zeros(len(y))\n",
    "    test_ens = np.zeros(len(test_scores))\n",
    "\n",
    "    for seed in seeds:\n",
    "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        oof_seed = np.zeros(len(y))\n",
    "        for tr, va in outer_cv.split(oof_scores, y, week_num):\n",
    "            meta = CalibratedClassifierCV(\n",
    "                RidgeClassifier(alpha=alpha), cv=inner_cv, method=\"sigmoid\")\n",
    "            meta.fit(oof_scores[tr], y[tr])\n",
    "            oof_seed[va] = meta.predict_proba(oof_scores[va])[:, 1]\n",
    "        oof_ens += oof_seed / len(seeds)\n",
    "\n",
    "        meta_full = CalibratedClassifierCV(\n",
    "            RidgeClassifier(alpha=alpha), cv=inner_cv, method=\"sigmoid\")\n",
    "        meta_full.fit(oof_scores, y)\n",
    "        test_ens += meta_full.predict_proba(test_scores)[:, 1] / len(seeds)\n",
    "        print(f\"  Seed {seed}: AUC={roc_auc_score(y, oof_seed):.6f}\")\n",
    "\n",
    "    oof_auc = roc_auc_score(y, oof_ens)\n",
    "    stab = gini_stability(week_num, y, oof_ens)\n",
    "    print(f\"\\n  Ensemble ({len(seeds)}-seed avg): \"\n",
    "          f\"AUC={oof_auc:.6f}  Stability={stab['stability_score']:.6f}\")\n",
    "    return {\"oof_preds\": oof_ens, \"test_preds\": test_ens, \"oof_auc\": oof_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH1_NAMES = [\n",
    "    \"applprev_1\", \"credit_bureau_a_1\", \"credit_bureau_b_1\",\n",
    "    \"person_1\", \"tax_registry_a_1\", \"tax_registry_b_1\", \"tax_registry_c_1\",\n",
    "]\n",
    "DEPTH2_NAMES = [\"applprev_2\", \"person_2\", \"credit_bureau_a_2\"]\n",
    "CLOSED_INDICATORS = [\n",
    "    \"dateofcredend_353D\", \"dateofcredstart_739D\",\n",
    "    \"credlmt_228A\", \"contractst_964M\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_split(split):\n",
    "    \"\"\"Load raw tables, preprocess, aggregate, and merge for one split.\"\"\"\n",
    "    base = load_table_group(DATA_PATH, \"base\", split=split)\n",
    "    df = base.clone()\n",
    "\n",
    "    for tg in [\"static_0\", \"static_cb_0\"]:\n",
    "        try:\n",
    "            df = df.join(\n",
    "                preprocess_table(load_table_group(DATA_PATH, tg, split=split)),\n",
    "                on=\"case_id\", how=\"left\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    df = handle_dates(df)\n",
    "    df = create_domain_ratios(df)\n",
    "    print(f\"  [{split}] depth-0: {df.shape}\")\n",
    "\n",
    "    for name in DEPTH1_NAMES:\n",
    "        try:\n",
    "            t = preprocess_table(load_table_group(DATA_PATH, name, split=split))\n",
    "            if name == \"credit_bureau_a_1\":\n",
    "                avail = [c for c in CLOSED_INDICATORS if c in t.columns]\n",
    "                if avail:\n",
    "                    t = t.filter(pl.col(avail[0]).is_not_null())\n",
    "            df = df.join(aggregate_depth1(t), on=\"case_id\", how=\"left\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    print(f\"  [{split}] + depth-1: {df.shape}\")\n",
    "\n",
    "    for name in DEPTH2_NAMES:\n",
    "        try:\n",
    "            t = preprocess_table(load_table_group(DATA_PATH, name, split=split))\n",
    "            df = df.join(aggregate_depth2(t), on=\"case_id\", how=\"left\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    print(f\"  [{split}] + depth-2: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Building train features …\")\n",
    "train = build_split(\"train\")\n",
    "train = drop_correlated_columns(train, threshold=0.95)\n",
    "train = collapse_rare_categories(train, max_unique=200, keep_top=20)\n",
    "print(f\"  Train final: {train.shape}\\n\")\n",
    "\n",
    "print(\"Building test features …\")\n",
    "test = build_split(\"test\")\n",
    "test = collapse_rare_categories(test, max_unique=200, keep_top=20)\n",
    "\n",
    "train_cols = [c for c in train.columns if c != \"target\"]\n",
    "missing = [c for c in train_cols if c not in test.columns]\n",
    "if missing:\n",
    "    print(f\"  Adding {len(missing)} null columns missing from test\")\n",
    "    test = test.with_columns([pl.lit(None).cast(train[c].dtype).alias(c) for c in missing])\n",
    "test = test.select(train_cols)\n",
    "print(f\"  Test final (aligned): {test.shape}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training — Tuned Hyperparameters, 5-Fold StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_COLS = {\"case_id\", \"target\", \"WEEK_NUM\"}\n",
    "feature_cols = [c for c in train.columns if c not in META_COLS]\n",
    "cat_cols = [c for c in feature_cols if train[c].dtype in (pl.String, pl.Utf8, pl.Categorical)]\n",
    "\n",
    "high_card = {c for c in cat_cols if train[c].n_unique() > 200}\n",
    "lgb_cat_cols = [c for c in cat_cols if c not in high_card]\n",
    "lgb_feature_cols = [c for c in feature_cols if c not in high_card]\n",
    "\n",
    "train_pd = train.to_pandas()\n",
    "test_pd = test.to_pandas()\n",
    "del train, test; gc.collect()\n",
    "\n",
    "X = train_pd[feature_cols]\n",
    "y = train_pd[\"target\"].values\n",
    "week_num = train_pd[\"WEEK_NUM\"].values\n",
    "\n",
    "X_lgb = train_pd[lgb_feature_cols].copy()\n",
    "X_test_lgb = test_pd[lgb_feature_cols].copy()\n",
    "for col in lgb_cat_cols:\n",
    "    X_lgb[col] = X_lgb[col].astype(\"category\")\n",
    "    X_test_lgb[col] = X_test_lgb[col].astype(\"category\")\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "print(f\"Features: {len(feature_cols)} total ({len(cat_cols)} cat)\")\n",
    "print(f\"LGB/XGB features: {len(lgb_feature_cols)} ({len(lgb_cat_cols)} cat, \"\n",
    "      f\"{len(high_card)} high-card excluded)\")\n",
    "print(f\"Train: {len(X):,}  Test: {len(test_pd):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CatBoost …\\n\")\n",
    "cb_params = TUNED_PARAMS[\"catboost\"]\n",
    "oof_cb = np.zeros(len(X))\n",
    "test_cb = np.zeros(len(test_pd))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X, y, week_num)):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000, **cb_params,\n",
    "        random_seed=SEED + fold, eval_metric=\"AUC\",\n",
    "        cat_features=cat_cols, allow_writing_files=False,\n",
    "    )\n",
    "    model.fit(\n",
    "        X.iloc[tr_idx], y[tr_idx],\n",
    "        eval_set=(X.iloc[va_idx], y[va_idx]),\n",
    "        early_stopping_rounds=100, verbose=0,\n",
    "    )\n",
    "    oof_cb[va_idx] = model.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "    test_cb += model.predict_proba(test_pd[feature_cols])[:, 1] / N_SPLITS\n",
    "    print(f\"  Fold {fold+1}: AUC={roc_auc_score(y[va_idx], oof_cb[va_idx]):.6f}\")\n",
    "\n",
    "stab = gini_stability(week_num, y, oof_cb)\n",
    "print(f\"\\n  CatBoost OOF: AUC={roc_auc_score(y, oof_cb):.6f}  \"\n",
    "      f\"Stability={stab['stability_score']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM …\\n\")\n",
    "lgb_params = TUNED_PARAMS[\"lightgbm\"]\n",
    "oof_lgb = np.zeros(len(X))\n",
    "test_lgb = np.zeros(len(test_pd))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X, y, week_num)):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000, **lgb_params,\n",
    "        random_state=SEED + fold, verbose=-1,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_lgb.iloc[tr_idx], y[tr_idx],\n",
    "        eval_set=[(X_lgb.iloc[va_idx], y[va_idx])],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "    )\n",
    "    oof_lgb[va_idx] = model.predict_proba(X_lgb.iloc[va_idx])[:, 1]\n",
    "    test_lgb += model.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n",
    "    print(f\"  Fold {fold+1}: AUC={roc_auc_score(y[va_idx], oof_lgb[va_idx]):.6f}\")\n",
    "\n",
    "stab = gini_stability(week_num, y, oof_lgb)\n",
    "print(f\"\\n  LightGBM OOF: AUC={roc_auc_score(y, oof_lgb):.6f}  \"\n",
    "      f\"Stability={stab['stability_score']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost (fold-safe encoding) …\\n\")\n",
    "xgb_params = TUNED_PARAMS[\"xgboost\"]\n",
    "oof_xgb = np.zeros(len(X))\n",
    "test_xgb = np.zeros(len(test_pd))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X, y, week_num)):\n",
    "    X_tr = train_pd[lgb_feature_cols].iloc[tr_idx].copy()\n",
    "    X_val = train_pd[lgb_feature_cols].iloc[va_idx].copy()\n",
    "\n",
    "    encoder = CatBoostEncoder(cols=lgb_cat_cols, random_state=SEED + fold)\n",
    "    X_tr[lgb_cat_cols] = encoder.fit_transform(X_tr[lgb_cat_cols], y[tr_idx])\n",
    "    X_val[lgb_cat_cols] = encoder.transform(X_val[lgb_cat_cols])\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=1000, **xgb_params,\n",
    "        random_state=SEED + fold, eval_metric=\"auc\",\n",
    "        tree_method=\"hist\", early_stopping_rounds=100, verbosity=0,\n",
    "    )\n",
    "    model.fit(X_tr, y[tr_idx], eval_set=[(X_val, y[va_idx])], verbose=0)\n",
    "\n",
    "    oof_xgb[va_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    X_te = test_pd[lgb_feature_cols].copy()\n",
    "    X_te[lgb_cat_cols] = encoder.transform(X_te[lgb_cat_cols])\n",
    "    test_xgb += model.predict_proba(X_te)[:, 1] / N_SPLITS\n",
    "\n",
    "    print(f\"  Fold {fold+1}: AUC={roc_auc_score(y[va_idx], oof_xgb[va_idx]):.6f}\")\n",
    "\n",
    "stab = gini_stability(week_num, y, oof_xgb)\n",
    "print(f\"\\n  XGBoost OOF: AUC={roc_auc_score(y, oof_xgb):.6f}  \"\n",
    "      f\"Stability={stab['stability_score']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stacking Ensemble & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking ensemble …\\n\")\n",
    "\n",
    "oof_stack = np.column_stack([oof_cb, oof_lgb, oof_xgb])\n",
    "test_stack = np.column_stack([test_cb, test_lgb, test_xgb])\n",
    "\n",
    "result = build_stacking_ensemble(\n",
    "    oof_stack, y, week_num, test_stack,\n",
    "    seeds=ENSEMBLE_SEEDS, alpha=ENSEMBLE_ALPHA,\n",
    "    n_splits=N_SPLITS, cv_seed=SEED,\n",
    ")\n",
    "\n",
    "# ── Build submission ──────────────────────────────────────────\n",
    "scores = np.clip(result[\"test_preds\"], 0.0, 1.0)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"case_id\": test_pd[\"case_id\"].astype(int),\n",
    "    \"score\": scores,\n",
    "})\n",
    "\n",
    "# Integrity checks\n",
    "assert list(submission.columns) == [\"case_id\", \"score\"], \"Bad columns\"\n",
    "assert submission[\"case_id\"].nunique() == len(submission), \"Duplicate case_id\"\n",
    "assert submission[\"score\"].notna().all(), \"Null scores\"\n",
    "assert (submission[\"score\"] >= 0).all() and (submission[\"score\"] <= 1).all(), \"Score OOB\"\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\nsubmission.csv saved  ({submission.shape[0]:,} rows)\")\n",
    "print(submission.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
