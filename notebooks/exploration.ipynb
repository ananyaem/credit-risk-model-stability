{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration\n",
        "\n",
        "Testing the data processing utilities with base table and static_0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from src.data_processing import (\n",
        "    load_table_group,\n",
        "    downcast_dtypes,\n",
        "    drop_high_missing_cols,\n",
        "    drop_high_cardinality_string_cols,\n",
        "    preprocess_table,\n",
        "    get_table_info,\n",
        ")\n",
        "from src.features import (\n",
        "    handle_dates, create_domain_ratios,\n",
        "    aggregate_depth1, aggregate_depth2,\n",
        "    drop_correlated_columns, collapse_rare_categories, remove_drift_features,\n",
        ")\n",
        "from src.metrics import gini_stability\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
        "plt.rcParams.update({\"figure.dpi\": 120, \"figure.facecolor\": \"white\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_PATH = \"../data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the base table\n",
        "base = load_table_group(DATA_PATH, \"base\", split=\"train\")\n",
        "print(f\"Base table shape: {base.shape}\")\n",
        "base.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check base table info\n",
        "get_table_info(base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocess base table\n",
        "base_processed = preprocess_table(base)\n",
        "print(f\"\\nAfter preprocessing: {base_processed.shape}\")\n",
        "get_table_info(base_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Static_0 Table\n",
        "\n",
        "This table has multiple chunks (static_0_0, static_0_1, etc.) that need to be concatenated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load static_0 - this will concatenate all chunks\n",
        "static_0 = load_table_group(DATA_PATH, \"static_0\", split=\"train\")\n",
        "print(f\"Static_0 table shape: {static_0.shape}\")\n",
        "static_0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check static_0 info before preprocessing\n",
        "info_before = get_table_info(static_0)\n",
        "print(f\"Shape: {info_before['shape']}\")\n",
        "print(f\"Memory: {info_before['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Dtype counts: {info_before['dtype_counts']}\")\n",
        "print(f\"Columns with >50% missing: {len(info_before['columns_with_high_missing'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test downcast_dtypes\n",
        "static_0_downcasted = downcast_dtypes(static_0)\n",
        "info_downcasted = get_table_info(static_0_downcasted)\n",
        "print(f\"Memory before downcast: {info_before['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Memory after downcast: {info_downcasted['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Memory reduction: {(1 - info_downcasted['estimated_memory_mb']/info_before['estimated_memory_mb'])*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test drop_high_missing_cols\n",
        "print(f\"Columns before: {static_0.shape[1]}\")\n",
        "static_0_no_missing = drop_high_missing_cols(static_0, threshold=0.98)\n",
        "print(f\"Columns after (threshold=0.98): {static_0_no_missing.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test drop_high_cardinality_string_cols\n",
        "static_0_no_high_card = drop_high_cardinality_string_cols(static_0, max_unique=10_000)\n",
        "print(f\"Columns after dropping high-cardinality strings: {static_0_no_high_card.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply full preprocessing pipeline\n",
        "static_0_processed = preprocess_table(static_0)\n",
        "print(f\"\\nFinal shape after full preprocessing: {static_0_processed.shape}\")\n",
        "get_table_info(static_0_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (a) Target Distribution & Temporal Drift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_counts = base[\"target\"].value_counts().sort(\"target\").to_pandas()\n",
        "total = target_counts[\"count\"].sum()\n",
        "default_rate = target_counts.loc[target_counts[\"target\"] == 1, \"count\"].values[0] / total\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(\n",
        "    target_counts[\"target\"].astype(str),\n",
        "    target_counts[\"count\"],\n",
        "    color=[\"#4C72B0\", \"#DD8452\"],\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.5,\n",
        ")\n",
        "for bar, count in zip(bars, target_counts[\"count\"]):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
        "            f\"{count:,}\\n({count/total:.1%})\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "ax.set_xlabel(\"Target\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(f\"Target Distribution (default rate = {default_rate:.2%})\")\n",
        "ax.ticklabel_format(axis=\"y\", style=\"plain\")\n",
        "\n",
        "weekly = (\n",
        "    base.group_by(\"WEEK_NUM\")\n",
        "    .agg([\n",
        "        pl.col(\"target\").mean().alias(\"default_rate\"),\n",
        "        pl.col(\"target\").count().alias(\"n_cases\"),\n",
        "    ])\n",
        "    .sort(\"WEEK_NUM\")\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(weekly[\"WEEK_NUM\"], weekly[\"default_rate\"], color=\"#4C72B0\", linewidth=1.2)\n",
        "z = np.polyfit(weekly[\"WEEK_NUM\"], weekly[\"default_rate\"], 1)\n",
        "ax.plot(weekly[\"WEEK_NUM\"], np.polyval(z, weekly[\"WEEK_NUM\"]),\n",
        "        \"--\", color=\"#DD8452\", linewidth=1.5, label=f\"trend (slope={z[0]:.5f})\")\n",
        "ax.set_xlabel(\"WEEK_NUM\")\n",
        "ax.set_ylabel(\"Default Rate\")\n",
        "ax.set_title(\"Default Rate by Week (temporal drift)\")\n",
        "ax.legend(fontsize=9)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(13, 3))\n",
        "ax.bar(weekly[\"WEEK_NUM\"], weekly[\"n_cases\"], color=\"#4C72B0\", edgecolor=\"none\", width=1.0)\n",
        "ax.set_xlabel(\"WEEK_NUM\")\n",
        "ax.set_ylabel(\"Number of Cases\")\n",
        "ax.set_title(\"Case Volume by Week\")\n",
        "ax.ticklabel_format(axis=\"y\", style=\"plain\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b) Missing Rates Across Table Groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TABLE_GROUPS = [\n",
        "    \"base\", \"static_0\", \"static_cb_0\",\n",
        "    \"person_1\", \"person_2\",\n",
        "    \"applprev_1\", \"applprev_2\",\n",
        "    \"credit_bureau_a_1\", \"credit_bureau_a_2\",\n",
        "    \"credit_bureau_b_1\", \"credit_bureau_b_2\",\n",
        "    \"debitcard_1\", \"deposit_1\", \"other_1\",\n",
        "    \"tax_registry_a_1\", \"tax_registry_b_1\", \"tax_registry_c_1\",\n",
        "]\n",
        "\n",
        "missing_summary = []\n",
        "for tg in TABLE_GROUPS:\n",
        "    try:\n",
        "        df = load_table_group(DATA_PATH, tg, split=\"train\")\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "    n = df.height\n",
        "    nc = df.null_count()\n",
        "    for col in df.columns:\n",
        "        if col == \"case_id\":\n",
        "            continue\n",
        "        rate = nc[col][0] / n\n",
        "        missing_summary.append({\"table_group\": tg, \"column\": col, \"missing_rate\": rate})\n",
        "\n",
        "missing_df = pl.DataFrame(missing_summary)\n",
        "print(f\"Total feature columns across all tables: {missing_df.height}\")\n",
        "print(f\"Columns with >50% missing: {missing_df.filter(pl.col('missing_rate') > 0.5).height}\")\n",
        "print(f\"Columns with >90% missing: {missing_df.filter(pl.col('missing_rate') > 0.9).height}\")\n",
        "print(f\"Columns with >98% missing: {missing_df.filter(pl.col('missing_rate') > 0.98).height}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "table_miss = (\n",
        "    missing_df.group_by(\"table_group\")\n",
        "    .agg([\n",
        "        pl.col(\"missing_rate\").mean().alias(\"avg_missing\"),\n",
        "        pl.col(\"missing_rate\").max().alias(\"max_missing\"),\n",
        "        (pl.col(\"missing_rate\") > 0.98).sum().alias(\"cols_gt_98pct\"),\n",
        "        pl.len().alias(\"n_cols\"),\n",
        "    ])\n",
        "    .sort(\"avg_missing\", descending=True)\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.barh(table_miss[\"table_group\"], table_miss[\"avg_missing\"], color=\"#4C72B0\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Average Missing Rate\")\n",
        "ax.set_title(\"Average Missing Rate per Table Group\")\n",
        "ax.invert_yaxis()\n",
        "ax.axvline(0.5, color=\"grey\", linestyle=\"--\", linewidth=0.8, alpha=0.7)\n",
        "\n",
        "ax = axes[1]\n",
        "colors = [\"#DD8452\" if v > 0 else \"#4C72B0\" for v in table_miss[\"cols_gt_98pct\"]]\n",
        "ax.barh(table_miss[\"table_group\"], table_miss[\"cols_gt_98pct\"], color=colors, edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Number of Columns\")\n",
        "ax.set_title(\"Columns with >98% Missing per Table Group\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_missing = (\n",
        "    missing_df.filter(pl.col(\"missing_rate\") > 0.90)\n",
        "    .sort(\"missing_rate\", descending=True)\n",
        ")\n",
        "print(f\"Columns with >90% missing ({top_missing.height} total):\")\n",
        "print(top_missing.head(30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (c) Feature Drift Detection\n",
        "\n",
        "Compare numeric feature distributions in **early weeks** (WEEK_NUM 0–30) vs **late weeks** (WEEK_NUM 61–91).\n",
        "For each feature we measure:\n",
        "- **Relative shift in mean**: `|mean_late - mean_early| / (std_overall + ε)`\n",
        "- **Relative shift in std**: `|std_late - std_early| / (std_overall + ε)`\n",
        "- **Shift in missing rate**: `|miss_late - miss_early|`\n",
        "\n",
        "Features with large shifts are candidates for dropping to improve model stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "static_with_week = static_0.join(\n",
        "    base.select(\"case_id\", \"WEEK_NUM\"), on=\"case_id\", how=\"left\"\n",
        ")\n",
        "\n",
        "numeric_cols = [\n",
        "    c for c in static_0.columns\n",
        "    if c != \"case_id\" and static_0[c].dtype in (pl.Float64, pl.Float32, pl.Int64, pl.Int32)\n",
        "]\n",
        "print(f\"Numeric columns to analyze: {len(numeric_cols)}\")\n",
        "\n",
        "EARLY_MAX = 30\n",
        "LATE_MIN = 61\n",
        "\n",
        "early = static_with_week.filter(pl.col(\"WEEK_NUM\") <= EARLY_MAX)\n",
        "late = static_with_week.filter(pl.col(\"WEEK_NUM\") >= LATE_MIN)\n",
        "print(f\"Early weeks (0-{EARLY_MAX}): {early.height:,} rows\")\n",
        "print(f\"Late weeks ({LATE_MIN}-91): {late.height:,} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EPS = 1e-9\n",
        "drift_records = []\n",
        "\n",
        "overall_stats = static_0.select([\n",
        "    pl.col(c).cast(pl.Float64).std().alias(f\"{c}__std\") for c in numeric_cols\n",
        "])\n",
        "\n",
        "for col in numeric_cols:\n",
        "    std_all = overall_stats[f\"{col}__std\"][0]\n",
        "    if std_all is None:\n",
        "        continue\n",
        "    std_all = float(std_all)\n",
        "\n",
        "    mean_e = early[col].cast(pl.Float64).mean()\n",
        "    mean_l = late[col].cast(pl.Float64).mean()\n",
        "    std_e = early[col].cast(pl.Float64).std()\n",
        "    std_l = late[col].cast(pl.Float64).std()\n",
        "    miss_e = early[col].null_count() / early.height\n",
        "    miss_l = late[col].null_count() / late.height\n",
        "\n",
        "    if mean_e is None or mean_l is None:\n",
        "        continue\n",
        "\n",
        "    mean_shift = abs(mean_l - mean_e) / (std_all + EPS)\n",
        "    std_shift = abs((std_l or 0) - (std_e or 0)) / (std_all + EPS)\n",
        "    miss_shift = abs(miss_l - miss_e)\n",
        "\n",
        "    drift_records.append({\n",
        "        \"column\": col,\n",
        "        \"mean_early\": round(mean_e, 4),\n",
        "        \"mean_late\": round(mean_l, 4),\n",
        "        \"mean_shift\": round(mean_shift, 4),\n",
        "        \"std_shift\": round(std_shift, 4),\n",
        "        \"miss_early\": round(miss_e, 4),\n",
        "        \"miss_late\": round(miss_l, 4),\n",
        "        \"miss_shift\": round(miss_shift, 4),\n",
        "    })\n",
        "\n",
        "drift_df = pl.DataFrame(drift_records).sort(\"mean_shift\", descending=True)\n",
        "print(f\"Analyzed {drift_df.height} numeric features for drift\")\n",
        "drift_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_n = 25\n",
        "top_drift = drift_df.head(top_n).to_pandas()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(17, 6))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.barh(top_drift[\"column\"], top_drift[\"mean_shift\"], color=\"#DD8452\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Normalised Mean Shift\")\n",
        "ax.set_title(f\"Top {top_n} Features by Mean Drift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "top_std = drift_df.sort(\"std_shift\", descending=True).head(top_n).to_pandas()\n",
        "ax = axes[1]\n",
        "ax.barh(top_std[\"column\"], top_std[\"std_shift\"], color=\"#55A868\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Normalised Std Shift\")\n",
        "ax.set_title(f\"Top {top_n} Features by Std Drift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "top_miss = drift_df.sort(\"miss_shift\", descending=True).head(top_n).to_pandas()\n",
        "ax = axes[2]\n",
        "ax.barh(top_miss[\"column\"], top_miss[\"miss_shift\"], color=\"#8172B2\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Δ Missing Rate\")\n",
        "ax.set_title(f\"Top {top_n} Features by Missing-Rate Shift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MEAN_SHIFT_THRESHOLD = 0.3\n",
        "STD_SHIFT_THRESHOLD = 0.3\n",
        "MISS_SHIFT_THRESHOLD = 0.1\n",
        "\n",
        "drift_flagged = drift_df.filter(\n",
        "    (pl.col(\"mean_shift\") > MEAN_SHIFT_THRESHOLD)\n",
        "    | (pl.col(\"std_shift\") > STD_SHIFT_THRESHOLD)\n",
        "    | (pl.col(\"miss_shift\") > MISS_SHIFT_THRESHOLD)\n",
        ").sort(\"mean_shift\", descending=True)\n",
        "\n",
        "print(f\"Features flagged for drift (any criterion): {drift_flagged.height}\")\n",
        "\n",
        "top_6 = drift_flagged.head(6)[\"column\"].to_list()\n",
        "if top_6:\n",
        "    n_plot = len(top_6)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 7))\n",
        "    axes = axes.flatten()\n",
        "    for i, col in enumerate(top_6):\n",
        "        ax = axes[i]\n",
        "        vals_e = early[col].drop_nulls().cast(pl.Float64).to_numpy()\n",
        "        vals_l = late[col].drop_nulls().cast(pl.Float64).to_numpy()\n",
        "        lo = np.nanpercentile(np.concatenate([vals_e, vals_l]), 1)\n",
        "        hi = np.nanpercentile(np.concatenate([vals_e, vals_l]), 99)\n",
        "        bins = np.linspace(lo, hi, 50)\n",
        "        ax.hist(vals_e, bins=bins, alpha=0.5, density=True, label=\"early\", color=\"#4C72B0\")\n",
        "        ax.hist(vals_l, bins=bins, alpha=0.5, density=True, label=\"late\", color=\"#DD8452\")\n",
        "        ax.set_title(col, fontsize=9)\n",
        "        ax.legend(fontsize=7)\n",
        "        ax.tick_params(labelsize=7)\n",
        "    for j in range(n_plot, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    fig.suptitle(\"Distribution Comparison: Early vs Late Weeks (top drifted features)\", fontsize=11)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (d) Candidate Drift-Prone Features to Drop\n",
        "\n",
        "Features are flagged if **any** of these hold:\n",
        "- Normalised mean shift > 0.3\n",
        "- Normalised std shift > 0.3\n",
        "- Missing rate shift > 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "high_missing_cols = (\n",
        "    missing_df.filter(\n",
        "        (pl.col(\"table_group\") == \"static_0\") & (pl.col(\"missing_rate\") > 0.98)\n",
        "    )[\"column\"].to_list()\n",
        ")\n",
        "\n",
        "drift_prone_cols = drift_flagged[\"column\"].to_list()\n",
        "\n",
        "candidates_to_drop = sorted(set(drift_prone_cols + high_missing_cols))\n",
        "\n",
        "print(f\"Drift-prone features (static_0): {len(drift_prone_cols)}\")\n",
        "print(f\"High-missing features (>98%, static_0): {len(high_missing_cols)}\")\n",
        "print(f\"Combined unique candidates to drop: {len(candidates_to_drop)}\")\n",
        "print()\n",
        "print(\"Candidate features to drop:\")\n",
        "for col in candidates_to_drop:\n",
        "    reasons = []\n",
        "    if col in drift_prone_cols:\n",
        "        reasons.append(\"drift\")\n",
        "    if col in high_missing_cols:\n",
        "        reasons.append(\">98% missing\")\n",
        "    print(f\"  {col:45s} [{', '.join(reasons)}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"DRIFT_PRONE_FEATURES = [\")\n",
        "for col in candidates_to_drop:\n",
        "    print(f'    \"{col}\",')\n",
        "print(\"]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Feature Engineering — Date Columns & Domain Ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `handle_dates`\n",
        "\n",
        "Join base (with `date_decision`) to static_0 (which has date columns ending in `D`), then convert all dates to numeric features relative to the decision date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged = base.join(static_0, on=\"case_id\", how=\"left\")\n",
        "\n",
        "date_d_cols = [c for c in merged.columns if c.endswith(\"D\") and c != \"date_decision\"]\n",
        "year_cols = [c for c in merged.columns if \"year\" in c.lower() and c not in date_d_cols and c != \"date_decision\"]\n",
        "\n",
        "print(f\"Columns ending in 'D' (excl. date_decision): {len(date_d_cols)}\")\n",
        "print(f\"  Examples: {date_d_cols[:5]}\")\n",
        "print(f\"Columns containing 'year': {len(year_cols)}\")\n",
        "if year_cols:\n",
        "    print(f\"  Examples: {year_cols[:5]}\")\n",
        "print(f\"\\nSample date_decision values:\\n{merged.select('date_decision').head(3)}\")\n",
        "if date_d_cols:\n",
        "    print(f\"\\nSample '{date_d_cols[0]}' values (before):\\n{merged.select(date_d_cols[0]).head(3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged_dates = handle_dates(merged)\n",
        "\n",
        "print(f\"Shape before: {merged.shape}\")\n",
        "print(f\"Shape after:  {merged_dates.shape}\")\n",
        "print(f\"\\n'date_decision' dropped: {'date_decision' not in merged_dates.columns}\")\n",
        "print(f\"'MONTH' dropped:         {'MONTH' not in merged_dates.columns}\")\n",
        "\n",
        "transformed_d_cols = [c for c in date_d_cols if c in merged_dates.columns]\n",
        "if transformed_d_cols:\n",
        "    sample_col = transformed_d_cols[0]\n",
        "    print(f\"\\nSample '{sample_col}' values (after — years before decision):\")\n",
        "    print(merged_dates.select(sample_col).head(5))\n",
        "    print(f\"\\n'{sample_col}' dtype after: {merged_dates[sample_col].dtype}\")\n",
        "\n",
        "if year_cols:\n",
        "    sample_year = year_cols[0]\n",
        "    print(f\"\\nSample '{sample_year}' values (after — delta from decision year):\")\n",
        "    print(merged_dates.select(sample_year).head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `create_domain_ratios`\n",
        "\n",
        "Compute loan burden, disbursement, debt, and interest-rate ratios from static_0 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_cols = [\"price_1097A\", \"annuity_780A\", \"disbursedcredamount_1113A\",\n",
        "               \"credamount_770A\", \"totaldebt_9A\", \"eir_270L\"]\n",
        "present = [c for c in source_cols if c in merged_dates.columns]\n",
        "missing = [c for c in source_cols if c not in merged_dates.columns]\n",
        "print(f\"Source columns present: {present}\")\n",
        "if missing:\n",
        "    print(f\"Source columns missing: {missing}\")\n",
        "\n",
        "merged_ratios = create_domain_ratios(merged_dates)\n",
        "\n",
        "ratio_cols = [\"loan_burden_ratio\", \"disbursed_credit_ratio\",\n",
        "              \"debt_credit_ratio\", \"eir_credit_ratio\"]\n",
        "new_cols = [c for c in ratio_cols if c in merged_ratios.columns]\n",
        "print(f\"\\nNew ratio columns created: {new_cols}\")\n",
        "print(f\"Shape before ratios: {merged_dates.shape}\")\n",
        "print(f\"Shape after ratios:  {merged_ratios.shape}\")\n",
        "\n",
        "if new_cols:\n",
        "    print(f\"\\nSample ratio values:\")\n",
        "    print(merged_ratios.select(new_cols).head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if new_cols:\n",
        "    n_plot = len(new_cols)\n",
        "    fig, axes = plt.subplots(1, n_plot, figsize=(5 * n_plot, 4))\n",
        "    if n_plot == 1:\n",
        "        axes = [axes]\n",
        "    for ax, col in zip(axes, new_cols):\n",
        "        vals = merged_ratios[col].drop_nulls().to_numpy()\n",
        "        lo, hi = np.nanpercentile(vals, [1, 99])\n",
        "        clipped = vals[(vals >= lo) & (vals <= hi)]\n",
        "        ax.hist(clipped, bins=50, color=\"#4C72B0\", edgecolor=\"none\", alpha=0.8)\n",
        "        ax.set_title(col, fontsize=10)\n",
        "        ax.set_ylabel(\"Count\")\n",
        "    fig.suptitle(\"Domain Ratio Feature Distributions (1st–99th pctl)\", fontsize=12)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Feature Engineering — Depth 1 Aggregations\n",
        "\n",
        "Load depth-1 tables, preprocess, aggregate by `case_id`, and join to the base table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DEPTH1_NAMES = [\n",
        "    \"applprev_1\",\n",
        "    \"credit_bureau_a_1\",\n",
        "    \"credit_bureau_b_1\",\n",
        "    \"person_1\",\n",
        "    \"tax_registry_a_1\",\n",
        "    \"tax_registry_b_1\",\n",
        "    \"tax_registry_c_1\",\n",
        "]\n",
        "\n",
        "depth1_tables = {}\n",
        "for name in DEPTH1_NAMES:\n",
        "    try:\n",
        "        df = load_table_group(DATA_PATH, name, split=\"train\")\n",
        "        df = preprocess_table(df)\n",
        "        depth1_tables[name] = df\n",
        "        print(f\"  {name:30s} {str(df.shape):>20s}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  {name:30s} {'NOT FOUND — skipped':>20s}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter `credit_bureau_a_1` to closed contracts\n",
        "\n",
        "Closed contracts populate closed-specific columns (e.g. `dateofcredend_353D`, `credlmt_228A`).\n",
        "Rows where these columns are **not null** represent closed contracts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CLOSED_INDICATORS = [\n",
        "    \"dateofcredend_353D\",\n",
        "    \"dateofcredstart_739D\",\n",
        "    \"credlmt_228A\",\n",
        "    \"contractst_964M\",\n",
        "]\n",
        "\n",
        "if \"credit_bureau_a_1\" in depth1_tables:\n",
        "    cb_a_1 = depth1_tables[\"credit_bureau_a_1\"]\n",
        "    available = [c for c in CLOSED_INDICATORS if c in cb_a_1.columns]\n",
        "\n",
        "    if available:\n",
        "        filter_col = available[0]\n",
        "        before = cb_a_1.height\n",
        "        cb_a_1_closed = cb_a_1.filter(pl.col(filter_col).is_not_null())\n",
        "        depth1_tables[\"credit_bureau_a_1\"] = cb_a_1_closed\n",
        "        print(f\"Filtered credit_bureau_a_1 on '{filter_col}' is_not_null:\")\n",
        "        print(f\"  {before:,} → {cb_a_1_closed.height:,} rows\")\n",
        "    else:\n",
        "        print(f\"Warning: no closed-contract indicator found in columns.\")\n",
        "        print(f\"  Searched for: {CLOSED_INDICATORS}\")\n",
        "        print(f\"  Using all {cb_a_1.height:,} rows without filtering.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregate each depth-1 table and join to base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "depth1_agg = {}\n",
        "for name, df in depth1_tables.items():\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"  {name}  (input shape: {df.shape})\")\n",
        "    print(f\"{'─'*60}\")\n",
        "    depth1_agg[name] = aggregate_depth1(df)\n",
        "\n",
        "print(f\"\\n{'═'*60}\")\n",
        "print(\"Summary:\")\n",
        "for name, agg_df in depth1_agg.items():\n",
        "    print(f\"  {name:30s} → {agg_df.shape[1] - 1:>5,} features, {agg_df.height:>8,} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "depth1_merged = base.clone()\n",
        "for name, agg_df in depth1_agg.items():\n",
        "    depth1_merged = depth1_merged.join(agg_df, on=\"case_id\", how=\"left\")\n",
        "    print(f\"  + {name:30s} → {depth1_merged.shape}\")\n",
        "\n",
        "total_d1_feats = depth1_merged.shape[1] - base.shape[1]\n",
        "print(f\"\\nBase columns:          {base.shape[1]}\")\n",
        "print(f\"New depth-1 features:  {total_d1_feats}\")\n",
        "print(f\"Final merged shape:    {depth1_merged.shape}\")\n",
        "print(f\"Memory:                {depth1_merged.estimated_size('mb'):.1f} MB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Feature Engineering — Depth 2 Aggregations\n",
        "\n",
        "Two-pass aggregation: first by `(case_id, num_group1)`, then by `case_id`.\n",
        "Skip `credit_bureau_b_2` (very high missing rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DEPTH2_NAMES = [\n",
        "    \"applprev_2\",\n",
        "    \"person_2\",\n",
        "    \"credit_bureau_a_2\",\n",
        "    # credit_bureau_b_2 skipped — very high missing rate\n",
        "]\n",
        "\n",
        "depth2_tables = {}\n",
        "for name in DEPTH2_NAMES:\n",
        "    try:\n",
        "        df = load_table_group(DATA_PATH, name, split=\"train\")\n",
        "        df = preprocess_table(df)\n",
        "        depth2_tables[name] = df\n",
        "        print(f\"  {name:30s} {str(df.shape):>20s}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  {name:30s} {'NOT FOUND — skipped':>20s}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "depth2_agg = {}\n",
        "for name, df in depth2_tables.items():\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"  {name}  (input shape: {df.shape})\")\n",
        "    print(f\"{'─'*60}\")\n",
        "    depth2_agg[name] = aggregate_depth2(df)\n",
        "\n",
        "print(f\"\\n{'═'*60}\")\n",
        "print(\"Summary:\")\n",
        "for name, agg_df in depth2_agg.items():\n",
        "    print(f\"  {name:30s} → {agg_df.shape[1] - 1:>5,} features, {agg_df.height:>8,} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Merge All Features (Depth 0 + 1 + 2)\n",
        "\n",
        "Combine depth-0 (base, static_0, static_cb_0), depth-1 aggregations, and depth-2 aggregations into a single training DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Depth-0 tables ──────────────────────────────────────────────\n",
        "train = base.join(static_0_processed, on=\"case_id\", how=\"left\")\n",
        "\n",
        "try:\n",
        "    static_cb_0 = load_table_group(DATA_PATH, \"static_cb_0\", split=\"train\")\n",
        "    static_cb_0 = preprocess_table(static_cb_0)\n",
        "    train = train.join(static_cb_0, on=\"case_id\", how=\"left\")\n",
        "    print(f\"+ static_cb_0          → {train.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"static_cb_0 not found — skipped\")\n",
        "\n",
        "train = handle_dates(train)\n",
        "train = create_domain_ratios(train)\n",
        "print(f\"Depth-0 (with dates & ratios): {train.shape}\")\n",
        "\n",
        "# ── Depth-1 aggregations ───────────────────────────────────────\n",
        "for name, agg_df in depth1_agg.items():\n",
        "    train = train.join(agg_df, on=\"case_id\", how=\"left\")\n",
        "print(f\"+ depth-1                    → {train.shape}\")\n",
        "\n",
        "# ── Depth-2 aggregations ───────────────────────────────────────\n",
        "for name, agg_df in depth2_agg.items():\n",
        "    train = train.join(agg_df, on=\"case_id\", how=\"left\")\n",
        "print(f\"+ depth-2                    → {train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = get_table_info(train)\n",
        "n_numeric = sum(1 for c in train.columns if train[c].dtype in (pl.Float32, pl.Float64, pl.Int32, pl.Int64))\n",
        "n_string = sum(1 for c in train.columns if train[c].dtype in (pl.String, pl.Utf8, pl.Categorical))\n",
        "\n",
        "print(f\"Final merged DataFrame\")\n",
        "print(f\"  Shape:           {train.shape}\")\n",
        "print(f\"  Memory:          {info['estimated_memory_mb']:.1f} MB\")\n",
        "print(f\"  Numeric cols:    {n_numeric}\")\n",
        "print(f\"  String cols:     {n_string}\")\n",
        "print(f\"  >50% missing:    {len(info['columns_with_high_missing'])}\")\n",
        "print(f\"\\nDtype breakdown: {info['dtype_counts']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Post-Merge Feature Filtering\n",
        "\n",
        "1. Drop columns that are >95% correlated (keep the one with lower missing rate)\n",
        "2. Collapse rare categories (>200 unique → keep top 20, rest to null)\n",
        "3. Remove drift-prone features identified in EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Before filtering: {train.shape}\")\n",
        "\n",
        "train = drop_correlated_columns(train, threshold=0.95)\n",
        "train = collapse_rare_categories(train, max_unique=200, keep_top=20)\n",
        "train = remove_drift_features(train, candidates_to_drop)\n",
        "\n",
        "print(f\"\\nAfter filtering:  {train.shape}\")\n",
        "info = get_table_info(train)\n",
        "print(f\"Memory:           {info['estimated_memory_mb']:.1f} MB\")\n",
        "print(f\"Dtype breakdown:  {info['dtype_counts']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Build Test Features & Save\n",
        "\n",
        "Replicate the full feature pipeline on the test split, align columns with the\n",
        "filtered train set, then save both as parquet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Depth-0 ─────────────────────────────────────────────────────\n",
        "test_base = load_table_group(DATA_PATH, \"base\", split=\"test\")\n",
        "test = test_base.clone()\n",
        "\n",
        "for tg in [\"static_0\", \"static_cb_0\"]:\n",
        "    try:\n",
        "        t = preprocess_table(load_table_group(DATA_PATH, tg, split=\"test\"))\n",
        "        test = test.join(t, on=\"case_id\", how=\"left\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "test = handle_dates(test)\n",
        "test = create_domain_ratios(test)\n",
        "print(f\"Test depth-0: {test.shape}\")\n",
        "\n",
        "# ── Depth-1 ─────────────────────────────────────────────────────\n",
        "for name in DEPTH1_NAMES:\n",
        "    try:\n",
        "        t = preprocess_table(load_table_group(DATA_PATH, name, split=\"test\"))\n",
        "        if name == \"credit_bureau_a_1\":\n",
        "            avail = [c for c in CLOSED_INDICATORS if c in t.columns]\n",
        "            if avail:\n",
        "                t = t.filter(pl.col(avail[0]).is_not_null())\n",
        "        test = test.join(aggregate_depth1(t), on=\"case_id\", how=\"left\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "print(f\"Test + depth-1: {test.shape}\")\n",
        "\n",
        "# ── Depth-2 ─────────────────────────────────────────────────────\n",
        "for name in DEPTH2_NAMES:\n",
        "    try:\n",
        "        t = preprocess_table(load_table_group(DATA_PATH, name, split=\"test\"))\n",
        "        test = test.join(aggregate_depth2(t), on=\"case_id\", how=\"left\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "print(f\"Test + depth-2: {test.shape}\")\n",
        "\n",
        "# ── Post-merge filtering ───────────────────────────────────────\n",
        "test = collapse_rare_categories(test, max_unique=200, keep_top=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align test columns with filtered train (minus target)\n",
        "train_feature_cols = [c for c in train.columns if c != \"target\"]\n",
        "present = [c for c in train_feature_cols if c in test.columns]\n",
        "missing_in_test = [c for c in train_feature_cols if c not in test.columns]\n",
        "\n",
        "if missing_in_test:\n",
        "    print(f\"Adding {len(missing_in_test)} null columns missing from test\")\n",
        "    test = test.with_columns([\n",
        "        pl.lit(None).cast(train[c].dtype).alias(c) for c in missing_in_test\n",
        "    ])\n",
        "\n",
        "test = test.select(train_feature_cols)\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test  shape: {test.shape}\")\n",
        "print(f\"Column match (excl. target): {list(test.columns) == train_feature_cols}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "\n",
        "out_dir = Path(DATA_PATH) / \"processed\"\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "train.write_parquet(out_dir / \"train_final.parquet\")\n",
        "test.write_parquet(out_dir / \"test_final.parquet\")\n",
        "\n",
        "print(f\"Saved to {out_dir.resolve()}\")\n",
        "print(f\"  train_final.parquet  {train.shape}  ({train.estimated_size('mb'):.1f} MB)\")\n",
        "print(f\"  test_final.parquet   {test.shape}  ({test.estimated_size('mb'):.1f} MB)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# CatBoost Baseline with StratifiedGroupKFold\n",
        "\n",
        "Train a CatBoost classifier using 5-fold CV where complete `WEEK_NUM` groups\n",
        "stay together (no week is split across folds). CatBoost handles categorical\n",
        "features natively — no encoding needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "META_COLS = {\"case_id\", \"target\", \"WEEK_NUM\"}\n",
        "feature_cols = [c for c in train.columns if c not in META_COLS]\n",
        "cat_cols = [c for c in feature_cols if train[c].dtype in (pl.String, pl.Utf8, pl.Categorical)]\n",
        "\n",
        "print(f\"Features:    {len(feature_cols)}\")\n",
        "print(f\"  numeric:   {len(feature_cols) - len(cat_cols)}\")\n",
        "print(f\"  categorical: {len(cat_cols)}\")\n",
        "\n",
        "train_pd = train.to_pandas()\n",
        "X = train_pd[feature_cols]\n",
        "y = train_pd[\"target\"].values\n",
        "week_num = train_pd[\"WEEK_NUM\"].values\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "oof_preds = np.zeros(len(X))\n",
        "fold_results = []\n",
        "cb_models = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y, week_num)):\n",
        "    print(f\"\\n{'═'*60}\")\n",
        "    print(f\"  Fold {fold + 1} / 5   \"\n",
        "          f\"(train {len(train_idx):,}  val {len(val_idx):,}  \"\n",
        "          f\"val weeks {np.unique(week_num[val_idx]).tolist()[:6]}…)\")\n",
        "    print(f\"{'═'*60}\")\n",
        "\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        l2_leaf_reg=3.0,\n",
        "        random_seed=42 + fold,\n",
        "        eval_metric=\"AUC\",\n",
        "        cat_features=cat_cols,\n",
        "        allow_writing_files=False,\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=(X_val, y_val),\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=200,\n",
        "    )\n",
        "\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    oof_preds[val_idx] = val_pred\n",
        "\n",
        "    fold_auc = roc_auc_score(y_val, val_pred)\n",
        "    fold_stab = gini_stability(week_num[val_idx], y_val, val_pred)\n",
        "\n",
        "    fold_results.append({\"fold\": fold + 1, \"auc\": fold_auc, **fold_stab})\n",
        "    cb_models.append(model)\n",
        "\n",
        "    print(f\"\\n  AUC:            {fold_auc:.6f}\")\n",
        "    print(f\"  Stability:      {fold_stab['stability_score']:.6f}\")\n",
        "    print(f\"  Mean Gini:      {fold_stab['mean_gini']:.6f}\")\n",
        "    print(f\"  Falling rate:   {fold_stab['falling_rate']:.6f}\")\n",
        "    print(f\"  Std residuals:  {fold_stab['std_residuals']:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "oof_auc = roc_auc_score(y, oof_preds)\n",
        "oof_stab = gini_stability(week_num, y, oof_preds)\n",
        "\n",
        "print(f\"{'═'*60}\")\n",
        "print(f\"  Overall OOF Results (CatBoost)\")\n",
        "print(f\"{'═'*60}\")\n",
        "print(f\"  AUC:            {oof_auc:.6f}\")\n",
        "print(f\"  Stability:      {oof_stab['stability_score']:.6f}\")\n",
        "print(f\"  Mean Gini:      {oof_stab['mean_gini']:.6f}\")\n",
        "print(f\"  Falling rate:   {oof_stab['falling_rate']:.6f}\")\n",
        "print(f\"  Std residuals:  {oof_stab['std_residuals']:.6f}\")\n",
        "print(f\"\\nPer-fold summary:\")\n",
        "for r in fold_results:\n",
        "    print(f\"  Fold {r['fold']}: AUC={r['auc']:.4f}  \"\n",
        "          f\"Stability={r['stability_score']:.4f}  \"\n",
        "          f\"Mean Gini={r['mean_gini']:.4f}\")\n",
        "\n",
        "ginis = oof_stab[\"weekly_ginis\"]\n",
        "x = np.arange(len(ginis))\n",
        "slope = oof_stab[\"slope\"]\n",
        "intercept = np.mean(ginis) - slope * np.mean(x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(13, 4))\n",
        "ax.plot(x, ginis, \"o-\", color=\"#4C72B0\", markersize=4, linewidth=1.2, label=\"weekly gini\")\n",
        "ax.plot(x, slope * x + intercept, \"--\", color=\"#DD8452\", linewidth=1.5,\n",
        "        label=f\"trend (slope={slope:.5f})\")\n",
        "ax.axhline(oof_stab[\"mean_gini\"], color=\"grey\", linestyle=\":\", linewidth=0.8,\n",
        "           label=f\"mean gini = {oof_stab['mean_gini']:.4f}\")\n",
        "ax.set_xlabel(\"Week Index\")\n",
        "ax.set_ylabel(\"Gini (2·AUC − 1)\")\n",
        "ax.set_title(f\"CatBoost OOF Weekly Gini  (stability = {oof_stab['stability_score']:.4f})\")\n",
        "ax.legend(fontsize=9)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "artifacts_dir = Path(\"..\") / \"artifacts\"\n",
        "artifacts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# OOF predictions\n",
        "oof_df = pl.DataFrame({\n",
        "    \"case_id\": train[\"case_id\"],\n",
        "    \"WEEK_NUM\": train[\"WEEK_NUM\"],\n",
        "    \"target\": train[\"target\"],\n",
        "    \"oof_score_catboost\": oof_preds,\n",
        "})\n",
        "oof_df.write_parquet(artifacts_dir / \"catboost_oof.parquet\")\n",
        "\n",
        "# Fold models\n",
        "for i, m in enumerate(cb_models):\n",
        "    m.save_model(str(artifacts_dir / f\"catboost_fold_{i}.cbm\"))\n",
        "\n",
        "# Scores\n",
        "with open(artifacts_dir / \"catboost_fold_scores.json\", \"w\") as f:\n",
        "    json.dump({\"fold_results\": fold_results, \"oof_auc\": oof_auc,\n",
        "               \"oof_stability\": oof_stab}, f, indent=2)\n",
        "\n",
        "print(f\"Artifacts saved to {artifacts_dir.resolve()}/\")\n",
        "print(f\"  catboost_oof.parquet          ({oof_df.shape})\")\n",
        "print(f\"  catboost_fold_0..4.cbm        (5 models)\")\n",
        "print(f\"  catboost_fold_scores.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}