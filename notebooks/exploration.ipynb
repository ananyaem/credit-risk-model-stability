{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration\n",
        "\n",
        "Testing the data processing utilities with base table and static_0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from src.data_processing import (\n",
        "    load_table_group,\n",
        "    downcast_dtypes,\n",
        "    drop_high_missing_cols,\n",
        "    drop_high_cardinality_string_cols,\n",
        "    preprocess_table,\n",
        "    get_table_info,\n",
        ")\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
        "plt.rcParams.update({\"figure.dpi\": 120, \"figure.facecolor\": \"white\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_PATH = \"../data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the base table\n",
        "base = load_table_group(DATA_PATH, \"base\", split=\"train\")\n",
        "print(f\"Base table shape: {base.shape}\")\n",
        "base.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check base table info\n",
        "get_table_info(base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocess base table\n",
        "base_processed = preprocess_table(base)\n",
        "print(f\"\\nAfter preprocessing: {base_processed.shape}\")\n",
        "get_table_info(base_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Static_0 Table\n",
        "\n",
        "This table has multiple chunks (static_0_0, static_0_1, etc.) that need to be concatenated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load static_0 - this will concatenate all chunks\n",
        "static_0 = load_table_group(DATA_PATH, \"static_0\", split=\"train\")\n",
        "print(f\"Static_0 table shape: {static_0.shape}\")\n",
        "static_0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check static_0 info before preprocessing\n",
        "info_before = get_table_info(static_0)\n",
        "print(f\"Shape: {info_before['shape']}\")\n",
        "print(f\"Memory: {info_before['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Dtype counts: {info_before['dtype_counts']}\")\n",
        "print(f\"Columns with >50% missing: {len(info_before['columns_with_high_missing'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test downcast_dtypes\n",
        "static_0_downcasted = downcast_dtypes(static_0)\n",
        "info_downcasted = get_table_info(static_0_downcasted)\n",
        "print(f\"Memory before downcast: {info_before['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Memory after downcast: {info_downcasted['estimated_memory_mb']:.2f} MB\")\n",
        "print(f\"Memory reduction: {(1 - info_downcasted['estimated_memory_mb']/info_before['estimated_memory_mb'])*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test drop_high_missing_cols\n",
        "print(f\"Columns before: {static_0.shape[1]}\")\n",
        "static_0_no_missing = drop_high_missing_cols(static_0, threshold=0.98)\n",
        "print(f\"Columns after (threshold=0.98): {static_0_no_missing.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test drop_high_cardinality_string_cols\n",
        "static_0_no_high_card = drop_high_cardinality_string_cols(static_0, max_unique=10_000)\n",
        "print(f\"Columns after dropping high-cardinality strings: {static_0_no_high_card.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply full preprocessing pipeline\n",
        "static_0_processed = preprocess_table(static_0)\n",
        "print(f\"\\nFinal shape after full preprocessing: {static_0_processed.shape}\")\n",
        "get_table_info(static_0_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (a) Target Distribution & Temporal Drift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_counts = base[\"target\"].value_counts().sort(\"target\").to_pandas()\n",
        "total = target_counts[\"count\"].sum()\n",
        "default_rate = target_counts.loc[target_counts[\"target\"] == 1, \"count\"].values[0] / total\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(\n",
        "    target_counts[\"target\"].astype(str),\n",
        "    target_counts[\"count\"],\n",
        "    color=[\"#4C72B0\", \"#DD8452\"],\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.5,\n",
        ")\n",
        "for bar, count in zip(bars, target_counts[\"count\"]):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
        "            f\"{count:,}\\n({count/total:.1%})\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "ax.set_xlabel(\"Target\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(f\"Target Distribution (default rate = {default_rate:.2%})\")\n",
        "ax.ticklabel_format(axis=\"y\", style=\"plain\")\n",
        "\n",
        "weekly = (\n",
        "    base.group_by(\"WEEK_NUM\")\n",
        "    .agg([\n",
        "        pl.col(\"target\").mean().alias(\"default_rate\"),\n",
        "        pl.col(\"target\").count().alias(\"n_cases\"),\n",
        "    ])\n",
        "    .sort(\"WEEK_NUM\")\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(weekly[\"WEEK_NUM\"], weekly[\"default_rate\"], color=\"#4C72B0\", linewidth=1.2)\n",
        "z = np.polyfit(weekly[\"WEEK_NUM\"], weekly[\"default_rate\"], 1)\n",
        "ax.plot(weekly[\"WEEK_NUM\"], np.polyval(z, weekly[\"WEEK_NUM\"]),\n",
        "        \"--\", color=\"#DD8452\", linewidth=1.5, label=f\"trend (slope={z[0]:.5f})\")\n",
        "ax.set_xlabel(\"WEEK_NUM\")\n",
        "ax.set_ylabel(\"Default Rate\")\n",
        "ax.set_title(\"Default Rate by Week (temporal drift)\")\n",
        "ax.legend(fontsize=9)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(13, 3))\n",
        "ax.bar(weekly[\"WEEK_NUM\"], weekly[\"n_cases\"], color=\"#4C72B0\", edgecolor=\"none\", width=1.0)\n",
        "ax.set_xlabel(\"WEEK_NUM\")\n",
        "ax.set_ylabel(\"Number of Cases\")\n",
        "ax.set_title(\"Case Volume by Week\")\n",
        "ax.ticklabel_format(axis=\"y\", style=\"plain\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b) Missing Rates Across Table Groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TABLE_GROUPS = [\n",
        "    \"base\", \"static_0\", \"static_cb_0\",\n",
        "    \"person_1\", \"person_2\",\n",
        "    \"applprev_1\", \"applprev_2\",\n",
        "    \"credit_bureau_a_1\", \"credit_bureau_a_2\",\n",
        "    \"credit_bureau_b_1\", \"credit_bureau_b_2\",\n",
        "    \"debitcard_1\", \"deposit_1\", \"other_1\",\n",
        "    \"tax_registry_a_1\", \"tax_registry_b_1\", \"tax_registry_c_1\",\n",
        "]\n",
        "\n",
        "missing_summary = []\n",
        "for tg in TABLE_GROUPS:\n",
        "    try:\n",
        "        df = load_table_group(DATA_PATH, tg, split=\"train\")\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "    n = df.height\n",
        "    nc = df.null_count()\n",
        "    for col in df.columns:\n",
        "        if col == \"case_id\":\n",
        "            continue\n",
        "        rate = nc[col][0] / n\n",
        "        missing_summary.append({\"table_group\": tg, \"column\": col, \"missing_rate\": rate})\n",
        "\n",
        "missing_df = pl.DataFrame(missing_summary)\n",
        "print(f\"Total feature columns across all tables: {missing_df.height}\")\n",
        "print(f\"Columns with >50% missing: {missing_df.filter(pl.col('missing_rate') > 0.5).height}\")\n",
        "print(f\"Columns with >90% missing: {missing_df.filter(pl.col('missing_rate') > 0.9).height}\")\n",
        "print(f\"Columns with >98% missing: {missing_df.filter(pl.col('missing_rate') > 0.98).height}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "table_miss = (\n",
        "    missing_df.group_by(\"table_group\")\n",
        "    .agg([\n",
        "        pl.col(\"missing_rate\").mean().alias(\"avg_missing\"),\n",
        "        pl.col(\"missing_rate\").max().alias(\"max_missing\"),\n",
        "        (pl.col(\"missing_rate\") > 0.98).sum().alias(\"cols_gt_98pct\"),\n",
        "        pl.len().alias(\"n_cols\"),\n",
        "    ])\n",
        "    .sort(\"avg_missing\", descending=True)\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.barh(table_miss[\"table_group\"], table_miss[\"avg_missing\"], color=\"#4C72B0\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Average Missing Rate\")\n",
        "ax.set_title(\"Average Missing Rate per Table Group\")\n",
        "ax.invert_yaxis()\n",
        "ax.axvline(0.5, color=\"grey\", linestyle=\"--\", linewidth=0.8, alpha=0.7)\n",
        "\n",
        "ax = axes[1]\n",
        "colors = [\"#DD8452\" if v > 0 else \"#4C72B0\" for v in table_miss[\"cols_gt_98pct\"]]\n",
        "ax.barh(table_miss[\"table_group\"], table_miss[\"cols_gt_98pct\"], color=colors, edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Number of Columns\")\n",
        "ax.set_title(\"Columns with >98% Missing per Table Group\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_missing = (\n",
        "    missing_df.filter(pl.col(\"missing_rate\") > 0.90)\n",
        "    .sort(\"missing_rate\", descending=True)\n",
        ")\n",
        "print(f\"Columns with >90% missing ({top_missing.height} total):\")\n",
        "print(top_missing.head(30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (c) Feature Drift Detection\n",
        "\n",
        "Compare numeric feature distributions in **early weeks** (WEEK_NUM 0–30) vs **late weeks** (WEEK_NUM 61–91).\n",
        "For each feature we measure:\n",
        "- **Relative shift in mean**: `|mean_late - mean_early| / (std_overall + ε)`\n",
        "- **Relative shift in std**: `|std_late - std_early| / (std_overall + ε)`\n",
        "- **Shift in missing rate**: `|miss_late - miss_early|`\n",
        "\n",
        "Features with large shifts are candidates for dropping to improve model stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "static_with_week = static_0.join(\n",
        "    base.select(\"case_id\", \"WEEK_NUM\"), on=\"case_id\", how=\"left\"\n",
        ")\n",
        "\n",
        "numeric_cols = [\n",
        "    c for c in static_0.columns\n",
        "    if c != \"case_id\" and static_0[c].dtype in (pl.Float64, pl.Float32, pl.Int64, pl.Int32)\n",
        "]\n",
        "print(f\"Numeric columns to analyze: {len(numeric_cols)}\")\n",
        "\n",
        "EARLY_MAX = 30\n",
        "LATE_MIN = 61\n",
        "\n",
        "early = static_with_week.filter(pl.col(\"WEEK_NUM\") <= EARLY_MAX)\n",
        "late = static_with_week.filter(pl.col(\"WEEK_NUM\") >= LATE_MIN)\n",
        "print(f\"Early weeks (0-{EARLY_MAX}): {early.height:,} rows\")\n",
        "print(f\"Late weeks ({LATE_MIN}-91): {late.height:,} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EPS = 1e-9\n",
        "drift_records = []\n",
        "\n",
        "overall_stats = static_0.select([\n",
        "    pl.col(c).cast(pl.Float64).std().alias(f\"{c}__std\") for c in numeric_cols\n",
        "])\n",
        "\n",
        "for col in numeric_cols:\n",
        "    std_all = overall_stats[f\"{col}__std\"][0]\n",
        "    if std_all is None:\n",
        "        continue\n",
        "    std_all = float(std_all)\n",
        "\n",
        "    mean_e = early[col].cast(pl.Float64).mean()\n",
        "    mean_l = late[col].cast(pl.Float64).mean()\n",
        "    std_e = early[col].cast(pl.Float64).std()\n",
        "    std_l = late[col].cast(pl.Float64).std()\n",
        "    miss_e = early[col].null_count() / early.height\n",
        "    miss_l = late[col].null_count() / late.height\n",
        "\n",
        "    if mean_e is None or mean_l is None:\n",
        "        continue\n",
        "\n",
        "    mean_shift = abs(mean_l - mean_e) / (std_all + EPS)\n",
        "    std_shift = abs((std_l or 0) - (std_e or 0)) / (std_all + EPS)\n",
        "    miss_shift = abs(miss_l - miss_e)\n",
        "\n",
        "    drift_records.append({\n",
        "        \"column\": col,\n",
        "        \"mean_early\": round(mean_e, 4),\n",
        "        \"mean_late\": round(mean_l, 4),\n",
        "        \"mean_shift\": round(mean_shift, 4),\n",
        "        \"std_shift\": round(std_shift, 4),\n",
        "        \"miss_early\": round(miss_e, 4),\n",
        "        \"miss_late\": round(miss_l, 4),\n",
        "        \"miss_shift\": round(miss_shift, 4),\n",
        "    })\n",
        "\n",
        "drift_df = pl.DataFrame(drift_records).sort(\"mean_shift\", descending=True)\n",
        "print(f\"Analyzed {drift_df.height} numeric features for drift\")\n",
        "drift_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_n = 25\n",
        "top_drift = drift_df.head(top_n).to_pandas()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(17, 6))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.barh(top_drift[\"column\"], top_drift[\"mean_shift\"], color=\"#DD8452\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Normalised Mean Shift\")\n",
        "ax.set_title(f\"Top {top_n} Features by Mean Drift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "top_std = drift_df.sort(\"std_shift\", descending=True).head(top_n).to_pandas()\n",
        "ax = axes[1]\n",
        "ax.barh(top_std[\"column\"], top_std[\"std_shift\"], color=\"#55A868\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Normalised Std Shift\")\n",
        "ax.set_title(f\"Top {top_n} Features by Std Drift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "top_miss = drift_df.sort(\"miss_shift\", descending=True).head(top_n).to_pandas()\n",
        "ax = axes[2]\n",
        "ax.barh(top_miss[\"column\"], top_miss[\"miss_shift\"], color=\"#8172B2\", edgecolor=\"none\")\n",
        "ax.set_xlabel(\"Δ Missing Rate\")\n",
        "ax.set_title(f\"Top {top_n} Features by Missing-Rate Shift\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MEAN_SHIFT_THRESHOLD = 0.3\n",
        "STD_SHIFT_THRESHOLD = 0.3\n",
        "MISS_SHIFT_THRESHOLD = 0.1\n",
        "\n",
        "drift_flagged = drift_df.filter(\n",
        "    (pl.col(\"mean_shift\") > MEAN_SHIFT_THRESHOLD)\n",
        "    | (pl.col(\"std_shift\") > STD_SHIFT_THRESHOLD)\n",
        "    | (pl.col(\"miss_shift\") > MISS_SHIFT_THRESHOLD)\n",
        ").sort(\"mean_shift\", descending=True)\n",
        "\n",
        "print(f\"Features flagged for drift (any criterion): {drift_flagged.height}\")\n",
        "\n",
        "top_6 = drift_flagged.head(6)[\"column\"].to_list()\n",
        "if top_6:\n",
        "    n_plot = len(top_6)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 7))\n",
        "    axes = axes.flatten()\n",
        "    for i, col in enumerate(top_6):\n",
        "        ax = axes[i]\n",
        "        vals_e = early[col].drop_nulls().cast(pl.Float64).to_numpy()\n",
        "        vals_l = late[col].drop_nulls().cast(pl.Float64).to_numpy()\n",
        "        lo = np.nanpercentile(np.concatenate([vals_e, vals_l]), 1)\n",
        "        hi = np.nanpercentile(np.concatenate([vals_e, vals_l]), 99)\n",
        "        bins = np.linspace(lo, hi, 50)\n",
        "        ax.hist(vals_e, bins=bins, alpha=0.5, density=True, label=\"early\", color=\"#4C72B0\")\n",
        "        ax.hist(vals_l, bins=bins, alpha=0.5, density=True, label=\"late\", color=\"#DD8452\")\n",
        "        ax.set_title(col, fontsize=9)\n",
        "        ax.legend(fontsize=7)\n",
        "        ax.tick_params(labelsize=7)\n",
        "    for j in range(n_plot, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    fig.suptitle(\"Distribution Comparison: Early vs Late Weeks (top drifted features)\", fontsize=11)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (d) Candidate Drift-Prone Features to Drop\n",
        "\n",
        "Features are flagged if **any** of these hold:\n",
        "- Normalised mean shift > 0.3\n",
        "- Normalised std shift > 0.3\n",
        "- Missing rate shift > 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "high_missing_cols = (\n",
        "    missing_df.filter(\n",
        "        (pl.col(\"table_group\") == \"static_0\") & (pl.col(\"missing_rate\") > 0.98)\n",
        "    )[\"column\"].to_list()\n",
        ")\n",
        "\n",
        "drift_prone_cols = drift_flagged[\"column\"].to_list()\n",
        "\n",
        "candidates_to_drop = sorted(set(drift_prone_cols + high_missing_cols))\n",
        "\n",
        "print(f\"Drift-prone features (static_0): {len(drift_prone_cols)}\")\n",
        "print(f\"High-missing features (>98%, static_0): {len(high_missing_cols)}\")\n",
        "print(f\"Combined unique candidates to drop: {len(candidates_to_drop)}\")\n",
        "print()\n",
        "print(\"Candidate features to drop:\")\n",
        "for col in candidates_to_drop:\n",
        "    reasons = []\n",
        "    if col in drift_prone_cols:\n",
        "        reasons.append(\"drift\")\n",
        "    if col in high_missing_cols:\n",
        "        reasons.append(\">98% missing\")\n",
        "    print(f\"  {col:45s} [{', '.join(reasons)}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"DRIFT_PRONE_FEATURES = [\")\n",
        "for col in candidates_to_drop:\n",
        "    print(f'    \"{col}\",')\n",
        "print(\"]\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}